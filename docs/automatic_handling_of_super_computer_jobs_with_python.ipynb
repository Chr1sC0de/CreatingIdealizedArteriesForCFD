{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Handling of Super Computer Jobs with Python\n",
    "When working with a large data set we want to be able to upload jobs to the super computer run them and then download them. The process can be done manually, however it can be quite time consuming and annoying. We can automate this process with python using the scp and paramiko libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko as po\n",
    "import scp\n",
    "import pathlib as pt\n",
    "import logging\n",
    "import multiprocessing as m\n",
    "from functools import wraps\n",
    "\n",
    "'''\n",
    "personal config file exampe config.py:\n",
    "\n",
    "host     = ****\n",
    "username = ****\n",
    "password = ****\n",
    "'''\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a client class which allows us to copy and paste files as well as run commands within our super computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Copy_a_to_b_decorator:\n",
    "\n",
    "    def __init__(self, path_a_type, path_b_type):\n",
    "        self.path_a_type = path_a_type\n",
    "        self.path_b_type = path_b_type\n",
    "\n",
    "    def __call__(self, function):\n",
    "\n",
    "        def decorator(obj: \"Client\", path_a: pt.Path, path_b: pt.Path, retries=0):\n",
    "            retries += 1\n",
    "            path_a = pt.Path(path_a)\n",
    "            path_b = pt.Path(path_b)\n",
    "            template = '%s %s To %s %s'%(self.path_a_type, path_a, self.path_b_type, path_b)\n",
    "            if retries == 1:\n",
    "                obj.logger.info('Copying %s'%template)\n",
    "            try:\n",
    "                function(obj, path_a, path_b, retries=retries)\n",
    "                obj.logger.info('Completed Copy %s'%template)\n",
    "                return True\n",
    "            except ConnectionAbortedError:\n",
    "                if not obj.ssh.get_transport().is_active():\n",
    "                    obj.connect()\n",
    "                obj.logger.info('Retrying Copy'%template)\n",
    "                if retries > obj.retry_limit:\n",
    "                    raise ConnectionError(\"Number of retries, %d, greater than retry limit %d\"% (retries, obj.retry_limit) )\n",
    "                obj.put(path_a, path_b, retries=retries)\n",
    "\n",
    "        return decorator\n",
    "\n",
    "class Client:\n",
    "\n",
    "    def __init__(self, host, username, password, port=22, retry_limit=10):\n",
    "\n",
    "        self.port        = port\n",
    "        self.host        = host\n",
    "        self.username    = username\n",
    "        self.password    = password\n",
    "        self.retry_limit = retry_limit\n",
    "        self.logger      = logging.getLogger(f\"{self.__class__.__name__}:{self.username}{self.host}\")\n",
    "        self.ssh         = po.SSHClient()\n",
    "        self.ssh.load_system_host_keys()\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self, *args, retries=0, **kwargs):\n",
    "        retries+=1\n",
    "        try:\n",
    "            self.ssh.connect(self.host, username=self.username, password=self.password)\n",
    "            self.transport = self.ssh.get_transport()\n",
    "            self.scp       = scp.SCPClient(self.transport)\n",
    "        except:\n",
    "            if retries > self.retry_limit:\n",
    "                raise ConnectionError(\n",
    "                    \"Number of retries, %d, greater than retry limit %d, \\\n",
    "                        cannot connect\"%(retries, self.retry_limit)\n",
    "                )\n",
    "            self.logger.info('Retrying Connect')\n",
    "            self.connect(retries=retries)\n",
    "\n",
    "    def disconnect(self):\n",
    "        self.ssh.close()\n",
    "        self.scp.close()\n",
    "        self.transport.close()\n",
    "\n",
    "    @Copy_a_to_b_decorator(\"Local\", \"Remote\")\n",
    "    def copy_local_to_remote(self, local_folder, remote_folder, retries=0):\n",
    "        self.scp.put(local_folder.as_posix(), remote_path=remote_folder.as_posix(),recursive=True)\n",
    "\n",
    "    @Copy_a_to_b_decorator(\"Remote\", \"Local\")\n",
    "    def copy_remote_to_local(self, remote_folder, local_folder, retries=0):\n",
    "        self.scp.get(remote_folder.as_posix(), local_path=local_folder.as_posix(), recursive=True)\n",
    "\n",
    "    # utiltiy function for deleting a remote directory\n",
    "    def delete_remote_directory(self, remote_folder):\n",
    "        remote_folder = pt.Path(remote_folder)\n",
    "        self.logger.info('Removing %s from %s'%(remote_folder,self.host))\n",
    "        self.ssh.exec_command(\n",
    "            \"rm -rf %s\"%(remote_folder.as_posix())\n",
    "        )\n",
    "        self.logger.info('Completed Removing %s from %s'%(remote_folder,self.host))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the client class let's create some helper functions which submit and download jobs. As we want to submit multiple jobs at once and manage those jobs we need to keep track of the job id and have some process keep track of the varying id's with a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def submit_job(folder: pt.Path, remote_destination: pt.Path, manager_dict: dict):\n",
    "\n",
    "    client   = Client(config.host, config.username, config.password)\n",
    "\n",
    "    client.copy_local_to_remote(folder, remote_destination)\n",
    "\n",
    "    # if on windows ensure that the shell script is dos\n",
    "    client.ssh.exec_command(\n",
    "        f\"cd {(remote_destination/folder.name).as_posix()}; dos2unix openfoam_job.sh\"\n",
    "    )\n",
    "    # submit the job\n",
    "    _, stdout, _ = client.ssh.exec_command(\n",
    "        f\"cd {(remote_destination/folder.name).as_posix()}; qsub openfoam_job.sh\"\n",
    "    )\n",
    "    # keep track job id\n",
    "    output = stdout.read().decode(\"utf-8\")\n",
    "    job_id = output.split(\".\")[0]\n",
    "    np.savez(log_path, job_id=job_id)\n",
    "    client.logger.info(f\"{folder.name} is running with JobID: {job_id}\")\n",
    "    \n",
    "    manager_dict[job_id] = remote_destination/folder.name\n",
    "\n",
    "    client.disconnect()\n",
    "\n",
    "\n",
    "def download_job(remote_folder: pt.Path, local_destination: pt.Path):\n",
    "\n",
    "    client = Client(config.host, config.username, config.password)\n",
    "    client.logger.info(f\"{remote_folder.name} has finished running\")\n",
    "    client.copy_remote_to_local(remote_folder, local_destination)\n",
    "    client.delete_remote_directory(remote_folder)\n",
    "    client.disconnect()\n"
   ]
  },
  {
   "source": [
    "Now lets process the jobs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the same naming scheme, however we will have a different parent folder\n",
    "local_main_folder  = pt.Path(\"../scripts/generate_steady_foam_cases/foam_cases\")\n",
    "remote_main_folder = pt.Path(\"/scratch/m45/cm5094/projects/bifurcation_steady_state\")\n",
    "\n",
    "client = Client(config.host, config.username, config.password)\n",
    "\n",
    "# get a list of all the cases in our job folder\n",
    "all_cases     = list(local_main_folder.glob(\"*\"))\n",
    "# create a shared object to keep track of all the running jobs\n",
    "running_cases = mp.Manager().dict()\n",
    "\n",
    "while True:\n",
    "\n",
    "    # copy over the desired number of cases to the super computer using different processors\n",
    "    if len(running_cases) < max_cases:\n",
    "        try:\n",
    "            folder = all_cases.pop()\n",
    "            p = mp.Process(target=submit_job, args=(folder, remote_main_folder, running_cases))\n",
    "            p.start()\n",
    "            p.join()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # for each running job check whether they are complete and copy them back to the local system\n",
    "    for job_id in list(running_cases.keys()):\n",
    "        remote_folder = running_cases[job_id]\n",
    "        # the file which submits the job should create a dummy file called completed.tmp hits is so that we do not overload the pbs server, by running a lot of qstats\n",
    "        _, stdout, _  = client.ssh.exec_command(\"cd %s; FILE=completed.tmp; [ -e \\\"./$FILE\\\" ] && echo 1 || echo 0\"%remote_folder.as_posix())\n",
    "        parsed_stdout = bool(int(stdout.read().decode(\"utf-8\")))\n",
    "        if parsed_stdout:i\n",
    "            # download the job\n",
    "            p             = mp.Process(\n",
    "                target=download_job, args=(remote_folder, local_main_folder))\n",
    "            p.start()\n",
    "            p.join()\n",
    "            # now remove the job from the dictionary\n",
    "            running_cases.pop(job_id)\n",
    "\n",
    "    # finally break the loop when all jobs have been run \n",
    "    if all([not len(all_cases), not len(running_cases)]):\n",
    "        break\n",
    "    \n",
    "client.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}